{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/tinyMLx/colabs/blob/master/5_9_9_FederatedLearning.ipynb","timestamp":1760992580562}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pujOZk0HbuDS"},"source":["# Dataset & model setup"]},{"cell_type":"code","metadata":{"id":"do7yY5OndbRk"},"source":["# Import modules\n","import os\n","import sys\n","import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import torch.nn.functional as F\n","import copy\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","import io\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"id":"x3Zp330P3gvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sleBEUqLcoZr"},"source":["# Load MNIST Dataset\n","d = './data'\n","if not os.path.exists(d):\n","    os.mkdir(d)\n","\n","trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n","train_set = datasets.MNIST(root=d, train=True, transform=trans, download=True)\n","test_set = datasets.MNIST(root=d, train=False, transform=trans, download=True)\n","\n","batch_size = 32\n","global_train_loader = torch.utils.data.DataLoader(\n","                 dataset=train_set,\n","                 batch_size=batch_size,\n","                 shuffle=True)\n","global_test_loader = torch.utils.data.DataLoader(\n","                dataset=test_set,\n","                batch_size=batch_size,\n","                shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjLFqabndU7W"},"source":["# Define MNIST model\n","class MLP(nn.Module):\n","\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGwFGzj-f03R"},"source":["# Client Creation"]},{"cell_type":"code","metadata":{"id":"YDAJZOAJf743"},"source":["n_clients = 10\n","\n","def create_client(client_id, local_dataset, batch_size=32):\n","    model = MLP().to(device)\n","    loader = torch.utils.data.DataLoader(\n","                  dataset=local_dataset,\n","                  batch_size=batch_size,\n","                  shuffle=True)\n","    return {\"client_id\": client_id,\n","            \"local_dataset\": loader,\n","            \"local_model\" : model,\n","            \"optimizer\": optim.SGD(model.parameters(), lr=0.01, momentum=0.9)}\n","\n","# Partition datapoints\n","local_datasets = [[] for i in range(n_clients)]\n","for i, datapoint in enumerate(train_set):\n","    local_datasets[i%n_clients].append(datapoint)\n","\n","# Create clients\n","clients = [create_client(i, local_datasets[i]) for i in range(n_clients)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITU_UujnhwMi"},"source":["# Federated Learning Training"]},{"cell_type":"code","source":["def client_load_model(client, model):\n","    client[\"local_model\"].load_state_dict(model.state_dict())\n","\n","def client_local_training(client):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = client[\"optimizer\"]\n","    dataset = client[\"local_dataset\"]\n","    model = client[\"local_model\"].to(device)\n","    model.train()\n","\n","    for batch_idx, (x, target) in enumerate(dataset):\n","        x, target = x.to(device), target.to(device)\n","        out = model(x)\n","        loss = criterion(out, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","def server_aggregate_models(clients):\n","    models = [x[\"local_model\"].state_dict() for x in clients]\n","    averaged_model = copy.deepcopy(models[0])\n","    with torch.no_grad():\n","        for k in averaged_model.keys():\n","            averaged_model[k] = sum(m[k] for m in models) / len(models)\n","    return averaged_model\n","\n","def evaluate(model, dataset):\n","    model.to(device)\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss()\n","    total_loss, total, correct = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for x, target in dataset:\n","            x, target = x.to(device), target.to(device)\n","            out = model(x)\n","            loss = criterion(out, target)\n","            _, pred_label = torch.max(out, 1)\n","            total_loss += loss.item()\n","            total += x.size(0)\n","            correct += (pred_label == target).sum().item()\n","\n","    avg_loss = total_loss / len(dataset)\n","    acc = correct / total\n","    print(f\"loss: {avg_loss:.4f}, acc: {acc:.4f}\")\n","    return avg_loss, acc\n","\n","def get_model_size(model):\n","    buffer = io.BytesIO()\n","    torch.save(model.state_dict(), buffer)\n","    size_MB = buffer.getbuffer().nbytes / 1e6\n","    return size_MB\n","\n","def federated_learning_loop(save_dir):\n","    # Set seed\n","    torch.manual_seed(42)\n","    global_model = MLP().to(device)\n","\n","    client_participation_fraction = 0.2\n","    rounds = 1000\n","    eval_interval = 10  # Measure costly metrics every X rounds\n","    save_interval = 50  # Save model every Y rounds\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    metrics = {\n","        \"round\": [],\n","        \"loss\": [],\n","        \"accuracy\": [],\n","        \"round_time\": [],\n","        \"model_size\": []\n","    }\n","    best_acc = 0.0\n","    best_loss = np.inf\n","\n","    for r in range(rounds):\n","        print(f\"\\nRound {r+1}\")\n","\n","        # Start the timer\n","        start_time = time.time()\n","\n","        # Broadcast global model to clients\n","        for c in clients:\n","            client_load_model(c, global_model)\n","\n","        # Selected clients perform local training\n","        for c in clients:\n","            if random.random() <= client_participation_fraction:\n","                client_local_training(c)\n","\n","        # Aggregate models from participating clients\n","        aggregated_model = server_aggregate_models(clients)\n","\n","        # Update global model\n","        global_model.load_state_dict(aggregated_model)\n","\n","        # Stop the timer before computing the metrics\n","        round_time = time.time() - start_time\n","\n","        # Evaluation\n","        loss, acc = evaluate(global_model, global_test_loader)\n","\n","        if (r + 1) % eval_interval == 0 or r == 0 or r == rounds - 1:\n","            model_size = get_model_size(global_model)\n","        else:\n","            model_size = metrics[\"model_size\"][-1]\n","\n","        # Save metrics for plotting\n","        metrics[\"round\"].append(r+1)\n","        metrics[\"round_time\"].append(round_time)\n","        metrics[\"loss\"].append(loss)\n","        metrics[\"accuracy\"].append(acc)\n","        metrics[\"model_size\"].append(model_size)\n","\n","        # Save model periodically\n","        if (r + 1) % save_interval == 0:\n","            torch.save(global_model.state_dict(), f\"{save_dir}/model_round_{r+1}.pth\")\n","            print(f\"Saved checkpoint: model_round_{r+1}.pth\")\n","\n","        # Save best model so far\n","        if acc > best_acc:\n","            best_acc = acc\n","            torch.save(global_model.state_dict(), f\"{save_dir}/best_acc_model.pth\")\n","            print(f\"New best acc model saved (acc={acc:.4f})\")\n","        if loss < best_loss:\n","            best_loss = loss\n","            torch.save(global_model.state_dict(), f\"{save_dir}/best_loss_model.pth\")\n","            print(f\"New best loss model saved (loss={loss:.4f})\")\n","\n","    # Save final model\n","    torch.save(global_model.state_dict(), f\"{save_dir}/final_model.pth\")\n","    print(\"Final model saved\")\n","\n","    return metrics"],"metadata":{"id":"o--LBhHZxaak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_metrics(metrics, filename):\n","    rounds = np.array(metrics[\"round\"])\n","    loss = np.array(metrics[\"loss\"])\n","    acc = np.array(metrics[\"accuracy\"])\n","    round_time = np.array(metrics[\"round_time\"])\n","    model_size = np.array(metrics[\"model_size\"])\n","\n","    plt.style.use('seaborn-v0_8-paper')\n","\n","    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n","    axes = axes.ravel()\n","\n","    # Loss\n","    axes[0].plot(rounds, loss, color='tab:blue', linewidth=2)\n","    axes[0].set_title(\"Global Model Loss\", fontsize=15)\n","    axes[0].set_xlabel(\"Round\", fontsize=13)\n","    axes[0].set_ylabel(\"Loss\", fontsize=13)\n","    axes[0].grid(True)\n","\n","    # Accuracy\n","    axes[1].plot(rounds, acc, color='tab:orange', linewidth=2)\n","    axes[1].set_title(\"Global Model Accuracy\", fontsize=15)\n","    axes[1].set_xlabel(\"Round\", fontsize=13)\n","    axes[1].set_ylabel(\"Accuracy\", fontsize=13)\n","    axes[1].grid(True)\n","\n","    # Round time\n","    axes[2].plot(rounds, round_time, color='tab:green', linewidth=2)\n","    axes[2].set_title(\"Round Duration\", fontsize=15)\n","    axes[2].set_xlabel(\"Round\", fontsize=13)\n","    axes[2].set_ylabel(\"Time (s)\", fontsize=13)\n","    axes[2].grid(True)\n","\n","    # Model size\n","    axes[3].plot(rounds, model_size, color='tab:red', linewidth=2)\n","    axes[3].set_title(\"Model Size\", fontsize=15)\n","    axes[3].set_xlabel(\"Round\", fontsize=13)\n","    axes[3].set_ylabel(\"Size (MB)\", fontsize=13)\n","    axes[3].grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n","    plt.show()\n","\n","    print(f\"Plot saved to {filename}\")"],"metadata":{"id":"gQoaCyGN0R47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize_metrics(metrics, filename):\n","    rounds = np.array(metrics[\"round\"])\n","    losses = np.array(metrics[\"loss\"])\n","    accs = np.array(metrics[\"accuracy\"])\n","    round_times = np.array(metrics[\"round_time\"])\n","    model_sizes = np.array(metrics[\"model_size\"])\n","\n","    mean_round_time = np.nanmean(round_times)\n","    std_round_time = np.nanstd(round_times)\n","    total_training_time = np.nansum(round_times)\n","\n","    final_loss = losses[-1]\n","    final_acc = accs[-1]\n","    best_loss = np.nanmin(losses)\n","    best_loss_round = rounds[np.nanargmin(losses)]\n","    best_acc = np.nanmax(accs)\n","    best_acc_round = rounds[np.nanargmax(accs)]\n","\n","    initial_model_size = model_sizes[0]\n","    final_model_size = model_sizes[-1]\n","    best_acc_model_size = model_sizes[np.nanargmax(accs)]\n","    best_loss_model_size = model_sizes[np.nanargmin(losses)]\n","\n","    size_reduction = initial_model_size - final_model_size\n","    size_reduction_pct = (size_reduction / initial_model_size) * 100 if initial_model_size > 0 else 0\n","    size_reduction_best_acc = initial_model_size - best_acc_model_size\n","    size_reduction_pct_best_acc = (size_reduction_best_acc / initial_model_size) * 100 if initial_model_size > 0 else 0\n","    size_reduction_best_loss = initial_model_size - best_loss_model_size\n","    size_reduction_pct_best_loss = (size_reduction_best_loss / initial_model_size) * 100 if initial_model_size > 0 else 0\n","\n","    acc_per_mb = best_acc / best_acc_model_size if best_acc_model_size > 0 else np.nan\n","\n","    summary = {\n","        \"final_loss\": float(final_loss),\n","        \"best_loss\": float(best_loss),\n","        \"final_accuracy\": float(final_acc),\n","        \"best_accuracy\": float(best_acc),\n","        \"best_loss_round\": int(best_loss_round),\n","        \"best_acc_round\": int(best_acc_round),\n","        \"mean_round_time\": float(mean_round_time),\n","        \"std_round_time\": float(std_round_time),\n","        \"total_training_time\": float(total_training_time),\n","        \"initial_model_size_MB\": float(initial_model_size),\n","        \"final_model_size_MB\": float(final_model_size),\n","        \"best_acc_model_size_MB\": float(best_acc_model_size),\n","        \"best_loss_model_size_MB\": float(best_loss_model_size),\n","        \"size_reduction_MB\": float(size_reduction),\n","        \"size_reduction_pct\": float(size_reduction_pct),\n","        \"size_reduction_best_acc_MB\": float(size_reduction_best_acc),\n","        \"size_reduction_pct_best_acc\": float(size_reduction_pct_best_acc),\n","        \"size_reduction_best_loss_MB\": float(size_reduction_best_loss),\n","        \"size_reduction_pct_best_loss\": float(size_reduction_pct_best_loss),\n","        \"accuracy_per_MB\": float(acc_per_mb)\n","    }\n","\n","    print(\"=== Training Summary ===\")\n","    print(f\"Final loss: {summary['final_loss']:.4f}\")\n","    print(f\"Best loss: {summary['best_loss']:.4f}\")\n","    print(f\"Final accuracy: {summary['final_accuracy']:.4f}\")\n","    print(f\"Best accuracy: {summary['best_accuracy']:.4f}\")\n","    print(f\"Best loss round: {summary['best_loss_round']}\")\n","    print(f\"Best accuracy round: {summary['best_acc_round']}\")\n","    print()\n","    print(f\"Mean round time: {summary['mean_round_time']:.3f} s\")\n","    print(f\"Std round time: {summary['std_round_time']:.3f} s\")\n","    print(f\"Total training time: {summary['total_training_time']:.3f} s\")\n","    print()\n","    print(f\"Initial model size: {summary['initial_model_size_MB']:.3f} MB\")\n","    print(f\"Final model size: {summary['final_model_size_MB']:.3f} MB\")\n","    print(f\"Model size at best accuracy: {summary['best_acc_model_size_MB']:.3f} MB\")\n","    print(f\"Model size at best loss: {summary['best_loss_model_size_MB']:.3f} MB\")\n","    print()\n","    print(f\"Size reduction (final): {summary['size_reduction_MB']:.3f} MB ({summary['size_reduction_pct']:.2f}%)\")\n","    print(f\"Size reduction (best accuracy): {summary['size_reduction_best_acc_MB']:.3f} MB ({summary['size_reduction_pct_best_acc']:.2f}%)\")\n","    print(f\"Size reduction (best loss): {summary['size_reduction_best_loss_MB']:.3f} MB ({summary['size_reduction_pct_best_loss']:.2f}%)\")\n","    print()\n","    print(f\"Accuracy per MB: {summary['accuracy_per_MB']:.4f}\")\n","\n","    with open(filename, \"w\") as f:\n","        json.dump(summary, f, indent=4)\n","\n","    print(f\"\\nSummary saved to {filename}\")\n","    return summary"],"metadata":{"id":"o1-RGZMCSsSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run loop\n","run_save_dir = \"checkpoints\"\n","metrics = federated_learning_loop(run_save_dir)"],"metadata":{"id":"E1E3v_2MHGr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot metrics\n","metrics_filename = \"training_metrics.png\"\n","plot_metrics(metrics, metrics_filename)"],"metadata":{"id":"EIdJ30MeL9JT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute summary statistics\n","summary_filename = \"summary.json\"\n","summary = summarize_metrics(metrics, summary_filename)"],"metadata":{"id":"3OIlTW0BSutL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/{run_save_dir}.zip /content/{run_save_dir}/\n","from google.colab import files\n","files.download(f\"/content/{run_save_dir}.zip\")\n","files.download(f\"/content/{metrics_filename}\")\n","files.download(f\"/content/{summary_filename}\")"],"metadata":{"collapsed":true,"id":"CmSGOKAjE-cp"},"execution_count":null,"outputs":[]}]}