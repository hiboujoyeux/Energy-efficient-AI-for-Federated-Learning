{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/tinyMLx/colabs/blob/master/5_9_9_FederatedLearning.ipynb","timestamp":1760992580562}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pujOZk0HbuDS"},"source":["# Dataset & model setup"]},{"cell_type":"code","metadata":{"id":"do7yY5OndbRk"},"source":["# Import modules\n","import os\n","import sys\n","import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import random\n","import torch.nn.functional as F\n","import copy\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"id":"x3Zp330P3gvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sleBEUqLcoZr"},"source":["# Load MNIST Dataset\n","d = './data'\n","if not os.path.exists(d):\n","    os.mkdir(d)\n","\n","trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n","train_set = datasets.MNIST(root=d, train=True, transform=trans, download=True)\n","test_set = datasets.MNIST(root=d, train=False, transform=trans, download=True)\n","\n","batch_size = 32\n","global_train_loader = torch.utils.data.DataLoader(\n","                 dataset=train_set,\n","                 batch_size=batch_size,\n","                 shuffle=True)\n","global_test_loader = torch.utils.data.DataLoader(\n","                dataset=test_set,\n","                batch_size=batch_size,\n","                shuffle=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjLFqabndU7W"},"source":["# Define MNIST model\n","class MLP(nn.Module):\n","\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGwFGzj-f03R"},"source":["# Client Creation"]},{"cell_type":"code","metadata":{"id":"YDAJZOAJf743"},"source":["n_clients = 10\n","\n","def create_client(client_id, local_dataset, batch_size=32):\n","    model = MLP().to(device)\n","    loader = torch.utils.data.DataLoader(\n","                  dataset=local_dataset,\n","                  batch_size=batch_size,\n","                  shuffle=True)\n","    return {\"client_id\": client_id,\n","            \"local_dataset\": loader,\n","            \"local_model\" : model,\n","            \"optimizer\": optim.SGD(model.parameters(), lr=0.01, momentum=0.9)}\n","\n","# Partition datapoints\n","local_datasets = [[] for i in range(n_clients)]\n","for i, datapoint in enumerate(train_set):\n","    local_datasets[i%n_clients].append(datapoint)\n","\n","# Create clients\n","clients = [create_client(i, local_datasets[i]) for i in range(n_clients)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITU_UujnhwMi"},"source":["# Federated Learning Training"]},{"cell_type":"code","source":["def client_load_model(client, model):\n","    client[\"local_model\"].load_state_dict(model.state_dict())\n","\n","def client_local_training(client):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = client[\"optimizer\"]\n","    dataset = client[\"local_dataset\"]\n","    model = client[\"local_model\"].to(device)\n","    model.train()\n","\n","    for batch_idx, (x, target) in enumerate(dataset):\n","        x, target = x.to(device), target.to(device)\n","        out = model(x)\n","        loss = criterion(out, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","def server_aggregate_models(clients):\n","    models = [x[\"local_model\"].state_dict() for x in clients]\n","    averaged_model = copy.deepcopy(models[0])\n","    with torch.no_grad():\n","        for k in averaged_model.keys():\n","            averaged_model[k] = sum(m[k] for m in models) / len(models)\n","    return averaged_model\n","\n","def evaluate(model, dataset):\n","    model.to(device)\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss()\n","    total_loss, total, correct = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for x, target in dataset:\n","            x, target = x.to(device), target.to(device)\n","            out = model(x)\n","            loss = criterion(out, target)\n","            _, pred_label = torch.max(out, 1)\n","            total_loss += loss.item()\n","            total += x.size(0)\n","            correct += (pred_label == target).sum().item()\n","\n","    avg_loss = total_loss / len(dataset)\n","    acc = correct / total\n","    print(f\"loss: {avg_loss:.4f}, acc: {acc:.4f}\")\n","    return avg_loss, acc\n","\n","def federated_learning_loop(save_dir):\n","    # Set seed\n","    torch.manual_seed(42)\n","    global_model = MLP().to(device)\n","\n","    client_participation_fraction = 0.2\n","    rounds = 1000\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    metrics = {\n","        \"round\": [],\n","        \"loss\": [],\n","        \"accuracy\": []\n","    }\n","    best_acc = 0.0\n","\n","    for r in range(rounds):\n","        print(f\"\\nRound {r+1}\")\n","\n","        # Broadcast global model to clients\n","        for c in clients:\n","            client_load_model(c, global_model)\n","\n","        # Selected clients perform local training\n","        for c in clients:\n","            if random.random() <= client_participation_fraction:\n","                client_local_training(c)\n","\n","        # Aggregate models from participating clients\n","        aggregated_model = server_aggregate_models(clients)\n","\n","        # Update global model\n","        global_model.load_state_dict(aggregated_model)\n","\n","        # Evaluation\n","        loss, acc = evaluate(global_model, global_test_loader)\n","\n","        # Save metrics for plotting\n","        metrics[\"round\"].append(r+1)\n","        metrics[\"loss\"].append(loss)\n","        metrics[\"accuracy\"].append(acc)\n","\n","        # Save model periodically\n","        save_every = 50\n","        if (r + 1) % save_every == 0:\n","            torch.save(global_model.state_dict(), f\"{save_dir}/model_round_{r+1}.pth\")\n","            print(f\"Saved checkpoint: model_round_{r+1}.pth\")\n","\n","        # Save best model so far\n","        if acc > best_acc:\n","            best_acc = acc\n","            torch.save(global_model.state_dict(), f\"{save_dir}/best_model.pth\")\n","            print(f\"New best model saved (acc={acc:.4f})\")\n","\n","    # Save final model\n","    torch.save(global_model.state_dict(), f\"{save_dir}/final_model.pth\")\n","    print(\"Final model saved\")\n","\n","    return metrics"],"metadata":{"id":"o--LBhHZxaak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run loop\n","save_dir = \"checkpoints\"\n","metrics = federated_learning_loop(save_dir)"],"metadata":{"id":"E1E3v_2MHGr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_metrics(metrics, filename):\n","    rounds = metrics[\"round\"]\n","    loss = np.array(metrics[\"loss\"])\n","    acc = np.array(metrics[\"accuracy\"])\n","\n","    plt.style.use('seaborn-v0_8-paper')\n","\n","    plt.figure(figsize=(12,5))\n","\n","    plt.subplot(1,2,1)\n","    plt.plot(rounds, loss, label=\"Loss\", color='tab:blue', linewidth=2)\n","    plt.xlabel(\"Round\", fontsize=14)\n","    plt.ylabel(\"Loss\", fontsize=14)\n","    plt.title(\"Global Model Loss\", fontsize=16)\n","    #plt.legend(fontsize=12)\n","    plt.grid(True)\n","\n","    plt.subplot(1,2,2)\n","    plt.plot(rounds, acc, label=\"Accuracy\", color='tab:orange', linewidth=2)\n","    plt.xlabel(\"Round\", fontsize=14)\n","    plt.ylabel(\"Accuracy\", fontsize=14)\n","    plt.title(\"Global Model Accuracy\", fontsize=16)\n","    #plt.legend(fontsize=12)\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(filename, dpi=300)\n","    plt.show()\n","    print(f\"Plot saved to {filename}\")"],"metadata":{"id":"gQoaCyGN0R47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting metrics\n","filename = \"training_metrics.png\"\n","plot_metrics(metrics, filename)"],"metadata":{"id":"EIdJ30MeL9JT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/{save_dir}.zip /content/{save_dir}/\n","from google.colab import files\n","files.download(f\"/content/{save_dir}.zip\")\n","files.download(f\"/content/{filename}\")"],"metadata":{"collapsed":true,"id":"CmSGOKAjE-cp"},"execution_count":null,"outputs":[]}]}